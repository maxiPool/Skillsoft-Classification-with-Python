{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cabbe93",
   "metadata": {},
   "source": [
    "## 3 DECISIONTREES/DECISIONTREES/DECISIONTREES DECISIONTREES 4 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of DecisionTrees - DecisionTrees for Tasks 1-6\n",
    "#### Task 1\n",
    "##### Import the required packages\n",
    "##### Set the working directory (home_dir, main_dir, data_dir, plot_dir)\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ba28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "# Set 'main_dir' to location of the project folder\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "plot_dir = str(main_dir) + \"/plots\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "print(plot_dir)\n",
    "print(plot_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11262231",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Read the dataset `heart_failure_clinical_records_dataset.csv` as `ex_df`\n",
    "##### Subset the dataframe to include only numeric and categorical columns\n",
    "##### Delete columns containing either 65% or more than 65% NaN Values\n",
    "##### Write a function to impute NA in both numeric and categorical columns\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d80a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset \n",
    "ex_df = pd.read_csv(str(data_dir)+\"/\"+ 'heart_failure_clinical_records_dataset.csv')\n",
    "# Subset data\n",
    "ex_df = ex_df[['age', 'serum_sodium', 'time', 'platelets', 'ejection_fraction', 'serum_creatinine', 'creatinine_phosphokinase', 'anaemia', 'high_blood_pressure', 'smoking', 'sex', 'diabetes', 'death_event', 'id']]\n",
    "# Delete columns containing either 65% or more than 65% NaN Values\n",
    "ex_perc = 65.0\n",
    "ex_min_count =  int(((100-ex_perc)/100)*ex_df.shape[0] + 1)\n",
    "ex_df = ex_df.dropna(axis=1, \n",
    "               thresh=ex_min_count)\n",
    "# Function to impute NA in both numeric and categorical columns\n",
    "def fillna(ex_df):\n",
    "    # Fill numeric columns with mean value\n",
    "    ex_df = ex_df.fillna(ex_df.mean())    \n",
    "    # Fill categorical columns with mode value\n",
    "    ex_df = ex_df.fillna(ex_df.mode().iloc[0])\n",
    "    return ex_df\n",
    "  \n",
    "ex_df = fillna(ex_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd72e1",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Identify the the two unique classes\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the the two unique classes\n",
    "ex_threshold = ex_df['death_event'].mean()\n",
    "ex_df['death_event'] = np.where(ex_df['death_event'] > ex_threshold, 1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451a79c",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### \n",
    "##### Split the data into ex_X and ex_y\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_unique_values = sorted(ex_df['death_event'].unique())\n",
    "ex_df['death_event'] = np.where(ex_df['death_event'] == ex_unique_values[0],  False,True)\n",
    "# Check class again.\n",
    "print(ex_df['death_event'].dtypes)\n",
    "# Split the data into ex_X and ex_y \n",
    "ex_columns_to_drop_from_X = ['death_event'] + ['id']\n",
    "ex_X = ex_df.drop(ex_columns_to_drop_from_X, axis = 1)\n",
    "ex_y = np.array(ex_df['death_event'])\n",
    "ex_X = pd.get_dummies(ex_X, columns = ['anaemia', 'high_blood_pressure', 'smoking', 'sex', 'diabetes'], dtype=float, drop_first=True)\n",
    "print(ex_X.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089af538",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Implement the Decision Tree and print it.\n",
    "##### Visualize `clf_fit_small`\n",
    "##### Split into train and test.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211110c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Decision Tree on ex_X.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_fit = clf.fit(ex_X, ex_y)\n",
    "print(clf_fit)\n",
    "# Visualize `clf_fit_small`\n",
    "tree.plot_tree(clf_fit, \n",
    "              feature_names= ex_X.columns,  \n",
    "              filled=True)\n",
    "plt.show()\n",
    "# Split into train and test.\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = train_test_split(ex_X, ex_y, test_size = 0.3)\n",
    "print(ex_X_train.shape, ex_y_train.shape)\n",
    "print(ex_X_test.shape, ex_y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360a975",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Implement the Decision Tree on ex_X_train\n",
    "##### Predict on ex_X_test\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cc9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Decision Tree on ex_X_train.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_fit = clf.fit(ex_X_train, ex_y_train)\n",
    "# Predict on ex_X_test.\n",
    "ex_y_predict = clf_fit.predict(ex_X_test)\n",
    "ex_y_predict[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f71e6",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of DecisionTrees - DecisionTrees for Task 7\n",
    "#### Task 7\n",
    "##### Compute test model accuracy score.\n",
    "##### Compute accuracy using training data.\n",
    "##### Calculate cv scores (accuracy) and print them.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test model accuracy score.\n",
    "ex_tree_accuracy_score = metrics.accuracy_score(ex_y_test, ex_y_predict)\n",
    "print(\"Accuracy on test data: \", ex_tree_accuracy_score)\n",
    "# Compute accuracy using training data.\n",
    "ex_acc_train_tree = clf_fit.score(ex_X_train,\n",
    "                                 ex_y_train)\n",
    "print (\"Train Accuracy:\", ex_acc_train_tree)\n",
    "ex_clf = tree.DecisionTreeClassifier()\n",
    "ex_cv_scores = cross_val_score(ex_clf, ex_X, ex_y, cv = 10)\n",
    "# Print each cv score (accuracy) and average them.\n",
    "print(ex_cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844229c",
   "metadata": {},
   "source": [
    "#### Please refer to module 4 of DecisionTrees - DecisionTrees for Tasks 8-12\n",
    "#### Task 8\n",
    "##### Define function that will determine the optimal number for each parameter.\n",
    "##### Optimize: max_depth\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that will determine the optimal number for each parameter.\n",
    "def optimal_parameter(values,test_results):\n",
    "    best_test_value = max(test_results)\n",
    "    best_test_index = test_results.index(best_test_value)\n",
    "    best_value = values[best_test_index]\n",
    "    return best_value\n",
    "# Fit a Decision Tree with depths ranging from 1 to 32 and plot the training and test accuracy\n",
    "ex_max_depths = np.linspace(1, 32, 32, endpoint=True, dtype=int)  # Convert to integer\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for max_depth in ex_max_depths:\n",
    "   ex_dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "   ex_dt.fit(ex_X_train, ex_y_train)\n",
    "   \n",
    "   ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "   ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "   \n",
    "   # Add accuracy score to previous train results\n",
    "   ex_train_results.append(ex_acc_train)\n",
    "   \n",
    "   ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "   ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "   \n",
    "   # Add accuracy score to previous test results\n",
    "   ex_test_results.append(ex_acc_test)\n",
    "# Store optimal max_depth.\n",
    "ex_optimal_max_depth = optimal_parameter(ex_max_depths, ex_test_results)  \n",
    "# Plot max depth over 1 - 32. \n",
    "line1, = plt.plot(ex_max_depths, ex_train_results, 'b', label=\"Train accuracy\")\n",
    "line2, = plt.plot(ex_max_depths, ex_test_results, 'r', label=\"Test accuracy\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368da41",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Optimize: min samples split\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac90f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for min_samples_split in ex_min_samples_splits:\n",
    "   ex_dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "   ex_dt.fit(ex_X_train, ex_y_train)\n",
    "   \n",
    "   ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "   ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "   \n",
    "   # Add accuracy score to previous train results\n",
    "   ex_train_results.append(ex_acc_train)\n",
    "   \n",
    "   ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "   ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "   \n",
    "   # Add accuracy score to previous test results\n",
    "   ex_test_results.append(ex_acc_test)\n",
    "# Store optimal max_depth.\n",
    "ex_optimal_min_samples_split = optimal_parameter(ex_min_samples_splits,ex_test_results) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bba230",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Optimize: Min_samples_leaf \n",
    "##### Optimize: max features\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe59d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min_samples_leaf:\n",
    "ex_min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint = True)\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for min_samples_leaf in ex_min_samples_leafs:\n",
    "   ex_dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "   ex_dt.fit(ex_X_train, ex_y_train)\n",
    "   \n",
    "   ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "   ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "   \n",
    "   # Add accuracy score to previous train results\n",
    "   ex_train_results.append(ex_acc_train)\n",
    "   \n",
    "   ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "   ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "   \n",
    "   # Add accuracy score to previous test results\n",
    "   ex_test_results.append(ex_acc_test)\n",
    "ex_optimal_min_samples_leafs = optimal_parameter(ex_min_samples_leafs,ex_test_results)\n",
    "# Optimize: max features\n",
    "ex_max_features = list(range(1,ex_X.shape[1]))\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for max_feature in ex_max_features:\n",
    "   ex_dt = DecisionTreeClassifier(max_features=max_feature)\n",
    "   ex_dt.fit(ex_X_train, ex_y_train)\n",
    "   \n",
    "   ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "   ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "   \n",
    "   # Add accuracy score to previous train results\n",
    "   ex_train_results.append(ex_acc_train)\n",
    "   \n",
    "   ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "   ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "   \n",
    "   # Add accuracy score to previous test results\n",
    "   ex_test_results.append(ex_acc_test)\n",
    "ex_optimal_max_features = optimal_parameter(ex_max_features,ex_test_results) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4d3b7",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Print: ex_optimal_max_depth,ex_optimal_min_samples_split, ex_optimal_min_samples_split, ex_optimal_max_features\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The optimal max depth is:\", ex_optimal_max_depth)\n",
    "print(\"The optimal min samples split is:\", ex_optimal_min_samples_split)\n",
    "print(\"The optimal min samples leaf is:\", ex_optimal_min_samples_split)\n",
    "print(\"The optimal max features is:\", ex_optimal_max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef579e5f",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Set the seed.\n",
    "##### Build Decision Tree classifier with optimized values calculated above\n",
    "##### Compute accuracy of the optimized model using training data\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimized model\n",
    "np.random.seed(1)\n",
    "# Implement the Decision Tree on ex_X_train.\n",
    "ex_clf_optimized = tree.DecisionTreeClassifier(max_depth = ex_optimal_max_depth,\n",
    "                                            min_samples_split = ex_optimal_min_samples_split,\n",
    "                                            min_samples_leaf = ex_optimal_min_samples_leafs,\n",
    "                                            max_features = ex_optimal_max_features)\n",
    "                                            \n",
    "# We can now see our optimized features where before they were just default:\n",
    "print(ex_clf_optimized)\n",
    "ex_clf_optimized_fit = ex_clf_optimized.fit(ex_X_train, ex_y_train)\n",
    "# Predict on ex_X_test.\n",
    "ex_y_predict_optimized = ex_clf_optimized_fit.predict(ex_X_test)\n",
    "# Get the accuracy score.\n",
    "ex_acc_score_tree_optimized = accuracy_score(ex_y_test, ex_y_predict_optimized)\n",
    "print(ex_acc_score_tree_optimized)\n",
    "# Compute accuracy using training data.\n",
    "ex_acc_train_tree_optimized = ex_clf_optimized_fit.score(ex_X_train, ex_y_train)\n",
    "print (\"Train Accuracy:\", ex_acc_train_tree_optimized)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
