{"cells":[{"cell_type":"code","execution_count":null,"id":"2c088b11","metadata":{"id":"2c088b11"},"outputs":[],"source":["#######################################################\n","#######################################################\n","############    COPYRIGHT - DATA SOCIETY   ############\n","#######################################################\n","#######################################################\n","\n","## 3 DECISIONTREES/DECISIONTREES/DECISIONTREES DECISIONTREES 2 ##\n","\n","## NOTE: To run individual pieces of code, select the line of code and\n","##       press ctrl + enter for PCs or command + enter for Macs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d8444e3f","metadata":{"id":"d8444e3f"},"outputs":[],"source":["#=================================================-\n","#### Slide 4: Loading packages  ####\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","from pathlib import Path\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.model_selection import GridSearchCV\n","\n","#import graphviz\n","from sklearn import tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_val_score\n","from matplotlib.legend_handler import HandlerLine2D\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ad947cef","metadata":{"id":"ad947cef"},"outputs":[],"source":["#=================================================-\n","#### Slide 5: Directory settings  ####\n","\n","# Set 'main_dir' to location of the project folder\n","home_dir = Path(\".\").resolve()\n","main_dir = home_dir.parent.parent\n","print(main_dir)\n","data_dir = str(main_dir) + \"/data\"\n","print(data_dir)\n","plot_dir = str(main_dir) + \"/plots\"\n","if not os.path.exists(plot_dir):\n","    os.makedirs(plot_dir)\n","print(plot_dir)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"564dbe79","metadata":{"id":"564dbe79"},"outputs":[],"source":["#=================================================-\n","#### Slide 6: Load the dataset  ####\n","\n","df = pd.read_csv(str(data_dir)+\"/\"+ 'healthcare-dataset-stroke-data.csv')\n","print(df.head())\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"072332f6","metadata":{"id":"072332f6"},"outputs":[],"source":["#=================================================-\n","#### Slide 7: Subset data  ####\n","\n","df = df[['age', 'avg_glucose_level', 'heart_disease', 'ever_married', 'hypertension', 'Residence_type', 'gender', 'smoking_status', 'work_type', 'stroke', 'id']]\n","print(df.head())\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"49baad6c","metadata":{"id":"49baad6c"},"outputs":[],"source":["#=================================================-\n","#### Slide 8: Data prep: check for NAs  ####\n","\n"," # Check for NAs.\n","print(df.isnull().sum())\n","percent_missing = df.isnull().sum() * 100 / len(df)\n","print(percent_missing)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"85b560f0","metadata":{"id":"85b560f0"},"outputs":[],"source":["#=================================================-\n","#### Slide 9: Data prep: check for NAs  ####\n","\n","# Delete columns containing either 50% or more than 50% NaN Values\n","perc = 50.0\n","min_count =  int(((100-perc)/100)*df.shape[0] + 1)\n","df = df.dropna(axis=1,\n","               thresh=min_count)\n","print(df.shape)\n","# Function to impute NA in both numeric and categorical columns\n","def fillna(df):\n","    numeric_columns = df.select_dtypes(include='number').columns\n","    df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean().to_dict())\n","\n","    categorical_columns = df.select_dtypes(exclude='number').columns\n","    df[categorical_columns] = df[categorical_columns].fillna(df[categorical_columns].mode().iloc[0])\n","\n","    return df\n","\n","df = fillna(df)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6123cae9","metadata":{"id":"6123cae9"},"outputs":[],"source":["#=================================================-\n","#### Slide 10: Data prep: target  ####\n","\n","print(df['stroke'].dtypes)\n","# Identify the the two unique classes\n","threshold = df['stroke'].mean()\n","df['stroke'] = np.where(df['stroke'] > threshold, 1,0)\n","unique_values = sorted(df['stroke'].unique())\n","df['stroke'] = np.where(df['stroke'] == unique_values[0],  False,True)\n","# Check class again.\n","print(df['stroke'].dtypes)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d5dd226b","metadata":{"id":"d5dd226b"},"outputs":[],"source":["#=================================================-\n","#### Slide 11: Summarize the data  ####\n","\n","print(df.describe())\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b518c7e6","metadata":{"id":"b518c7e6"},"outputs":[],"source":["#=================================================-\n","#### Slide 15: Decision Tree: splitting the data  ####\n","\n","# Split the data into X and y\n","columns_to_drop_from_X = ['stroke'] + ['id']\n","X = df.drop(columns_to_drop_from_X, axis = 1)\n","y = np.array(df['stroke'])\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b24b4f25","metadata":{"id":"b24b4f25"},"outputs":[],"source":["#=================================================-\n","#### Slide 16: Data prep: numeric variables  ####\n","\n","X = pd.get_dummies(X, columns = ['heart_disease', 'ever_married', 'hypertension', 'Residence_type', 'gender', 'smoking_status', 'work_type'], dtype=float, drop_first=True)\n","print(X.dtypes)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"16629827","metadata":{"id":"16629827"},"outputs":[],"source":["#=================================================-\n","#### Slide 17: Decision Tree: running the algorithm  ####\n","\n","# Implement the decision tree on X.\n","clf = tree.DecisionTreeClassifier()\n","clf_fit = clf.fit(X, y)\n","\n","# Look at our generated model:\n","print(clf_fit)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"10b671f3","metadata":{"id":"10b671f3"},"outputs":[],"source":["#=================================================-\n","#### Slide 18: Visualize: plot_tree  ####\n","\n","# Set figure size\n","fig = plt.figure(figsize=(25,20))\n","# Visualize `clf_fit_small`\n","tree.plot_tree(clf_fit,\n","              feature_names= X.columns,\n","              filled=True)\n","# Save figure\n","plt.savefig(str(plot_dir)+'/tree.png',format='png',bbox_inches = \"tight\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0903d518","metadata":{"id":"0903d518"},"outputs":[],"source":["#=================================================-\n","#### Slide 21: Split into train and test sets  ####\n","\n","# Split into train and test.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"768ba667","metadata":{"id":"768ba667"},"outputs":[],"source":["#=================================================-\n","#### Slide 22: Fit Decision Tree and predict  ####\n","\n","# Implement the decision tree on X_train.\n","clf = tree.DecisionTreeClassifier()\n","clf_fit = clf.fit(X_train, y_train)\n","\n","# Predict on X_test.\n","y_predict = clf_fit.predict(X_test)\n","y_predict[:20]\n","\n","\n","#######################################################\n","####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n","#######################################################\n"]},{"cell_type":"code","execution_count":null,"id":"57f10367","metadata":{"id":"57f10367"},"outputs":[],"source":["#######################################################\n","#######################################################\n","############    COPYRIGHT - DATA SOCIETY   ############\n","#######################################################\n","#######################################################\n","\n","## 3 DECISIONTREES/DECISIONTREES/DECISIONTREES DECISIONTREES 3 ##\n","\n","## NOTE: To run individual pieces of code, select the line of code and\n","##       press ctrl + enter for PCs or command + enter for Macs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"fdbf51df","metadata":{"id":"fdbf51df"},"outputs":[],"source":["#=================================================-\n","#### Slide 5: Evaluate the model (cont'd)  ####\n","\n","# Confusion matrix for first model.\n","cm_tree = confusion_matrix(y_test,y_predict)\n","# Accuracy score.\n","acc_score = accuracy_score(y_test, y_predict)\n","print(acc_score)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6909585a","metadata":{"id":"6909585a"},"outputs":[],"source":["#=================================================-\n","#### Slide 6: Plot confusion matrix  ####\n","\n","plt.clf()\n","plt.imshow(cm_tree, interpolation='nearest', cmap=plt.cm.Wistia)\n","classNames = ['Negative','Positive']\n","plt.title('Confusion Matrix - Test Data')\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","tick_marks = np.arange(len(classNames))\n","plt.xticks(tick_marks, classNames, rotation=45)\n","plt.yticks(tick_marks, classNames)\n","s = [['TN','FP'], ['FN', 'TP']]\n","for i in range(2):\n","    for j in range(2):\n","        plt.text(j,i, str(s[i][j]) + \" = \" + str(cm_tree[i][j]))\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"60c02742","metadata":{"id":"60c02742"},"outputs":[],"source":["#=================================================-\n","#### Slide 7: Plot ROC and calculate AUC  ####\n","\n","\n","# Calculate metrics for ROC (fpr, tpr) and calculate AUC.\n","fpr, tpr, threshold = metrics.roc_curve(y_test, y_predict)\n","roc_auc = metrics.auc(fpr, tpr)\n","\n","# Plot ROC.\n","plt.title('Receiver Operating Characteristic')\n","plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n","plt.legend(loc = 'lower right')\n","plt.plot([0, 1], [0, 1],'r--')\n","plt.xlim([0, 1])\n","plt.ylim([0, 1])\n","plt.ylabel('True Positive Rate')\n","plt.xlabel('False Positive Rate')\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f3eb8c3f","metadata":{"id":"f3eb8c3f"},"outputs":[],"source":["#=================================================-\n","#### Slide 9: Decision Tree: build  ####\n","\n","# Set up logistic regression model.\n","clf = tree.DecisionTreeClassifier()\n","print(clf)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2df868ab","metadata":{"id":"2df868ab"},"outputs":[],"source":["#=================================================-\n","#### Slide 10: Decision Tree: fit  ####\n","\n","# Fit the model.\n","clf_fit = clf.fit(X_train, y_train)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4fcea204","metadata":{"id":"4fcea204"},"outputs":[],"source":["#=================================================-\n","#### Slide 11: Decision Tree: predict  ####\n","\n","# Predict on X_test.\n","y_predict = clf_fit.predict(X_test)\n","print(y_predict[:20])\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5adf27cf","metadata":{"id":"5adf27cf"},"outputs":[],"source":["#=================================================-\n","#### Slide 12: Decision Tree: accuracy score  ####\n","\n","# Compute test model accuracy score.\n","tree_accuracy_score = metrics.accuracy_score(y_test, y_predict)\n","print(\"Accuracy on test data: \", tree_accuracy_score)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5abd7046","metadata":{"id":"5abd7046"},"outputs":[],"source":["#=================================================-\n","#### Slide 13: Decision Tree: train accuracy  ####\n","\n","# Compute accuracy using training data.\n","acc_train_tree = clf_fit.score(X_train,\n","                                 y_train)\n","print (\"Train Accuracy:\", acc_train_tree)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a081c2bf","metadata":{"id":"a081c2bf"},"outputs":[],"source":["#=================================================-\n","#### Slide 14: Decision Tree: accuracy  ####\n","\n","# Save this model to use later if needed\n","model_final_tree = {'metrics' : \"accuracy\" ,\n","                                  'values' : round(tree_accuracy_score,4),\n","                                  'model':'tree_all_variables' }\n","print(model_final_tree)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"d394859a","metadata":{"id":"d394859a"},"outputs":[],"source":["#=================================================-\n","#### Slide 20: Cross-validation scores  ####\n","\n","clf = tree.DecisionTreeClassifier()\n","cv_scores = cross_val_score(clf, X, y, cv = 10)\n","# Print each cv score (accuracy) and average them.\n","print(cv_scores)\n","print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))\n","mean = np.mean(cv_scores)\n","print(\"Optimal cv score is:\", round(mean, 4))\n","\n","\n","#######################################################\n","####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n","#######################################################\n"]},{"cell_type":"code","execution_count":null,"id":"f5816351","metadata":{"id":"f5816351"},"outputs":[],"source":["#######################################################\n","#######################################################\n","############    COPYRIGHT - DATA SOCIETY   ############\n","#######################################################\n","#######################################################\n","\n","## 3 DECISIONTREES/DECISIONTREES/DECISIONTREES DECISIONTREES 4 ##\n","\n","## NOTE: To run individual pieces of code, select the line of code and\n","##       press ctrl + enter for PCs or command + enter for Macs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e1252e74","metadata":{"id":"e1252e74"},"outputs":[],"source":["#=================================================-\n","#### Slide 4: Define an optimal number function  ####\n","\n","# Define function that will determine the optimal number for each parameter.\n","def optimal_parameter(values,test_results):\n","    best_test_value = max(test_results)\n","    best_test_index = test_results.index(best_test_value)\n","    best_value = values[best_test_index]\n","    return(best_value)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4b4b9c11","metadata":{"id":"4b4b9c11"},"outputs":[],"source":["#=================================================-\n","#### Slide 6: Optimize: max depth  ####\n","\n","# Max depth:\n","max_depths = range(1, 33)\n","train_results = []\n","test_results = []\n","\n","for max_depth in max_depths:\n","    dt = DecisionTreeClassifier(max_depth=max_depth)\n","    dt.fit(X_train, y_train)\n","\n","    train_pred = dt.predict(X_train)\n","    acc_train = accuracy_score(y_train, train_pred)\n","    train_results.append(acc_train)\n","\n","    y_pred = dt.predict(X_test);\n","    acc_test = accuracy_score(y_test, y_pred)\n","    test_results.append(acc_test);\n","# Store optimal max_depth.\n","optimal_max_depth = optimal_parameter(max_depths, test_results);\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4b976c38","metadata":{"id":"4b976c38"},"outputs":[],"source":["#=================================================-\n","#### Slide 7: Plot: max depth  ####\n","\n","# Plot max depth over 1 - 32.\n","line1, = plt.plot(max_depths, train_results, 'b', label= \"Train accuracy\")\n","line2, = plt.plot(max_depths, test_results, 'r', label= \"Test accuracy\")\n","\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints = 2)})\n","plt.ylabel('Accuracy')\n","plt.xlabel('Tree depth')\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"788d8376","metadata":{"id":"788d8376"},"outputs":[],"source":["#=================================================-\n","#### Slide 9: Optimize: min samples split  ####\n","\n","min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n","train_results = []\n","test_results = []\n","\n","for min_samples_split in min_samples_splits:\n","   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n","   dt.fit(X_train, y_train)\n","   train_pred = dt.predict(X_train)\n","   acc_train = accuracy_score(y_train, train_pred)\n","   # Add accuracy score to previous train results\n","   train_results.append(acc_train)\n","   y_pred = dt.predict(X_test)\n","   acc_test = accuracy_score(y_test, y_pred)\n","   # Add accuracy score to previous test results\n","   test_results.append(acc_test)\n","# Store optimal max_depth.\n","optimal_min_samples_split = optimal_parameter(min_samples_splits,test_results)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"97977261","metadata":{"id":"97977261"},"outputs":[],"source":["#=================================================-\n","#### Slide 10: Plot: min samples split  ####\n","\n","# Plot min_sample split.\n","line1, = plt.plot(min_samples_splits, train_results, 'b', label = \"Train accuracy\")\n","line2, = plt.plot(min_samples_splits, test_results, 'r', label = \"Test accuracy\")\n","\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n","plt.ylabel('Accuracy')\n","plt.xlabel('min samples split')\n","plt.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"85c39bd9","metadata":{"id":"85c39bd9"},"outputs":[],"source":["#=================================================-\n","#### Slide 12: Optimize: min samples leaf  ####\n","\n","# Min_samples_leaf:\n","min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint = True)\n","train_results = []\n","test_results = []\n","\n","for min_samples_leaf in min_samples_leafs:\n","   dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n","   dt.fit(X_train, y_train)\n","   train_pred = dt.predict(X_train)\n","   acc_train = accuracy_score(y_train, train_pred)\n","   # Add accuracy score to previous train results\n","   train_results.append(acc_train)\n","   y_pred = dt.predict(X_test)\n","   acc_test = accuracy_score(y_test, y_pred)\n","   # Add accuracy score to previous test results\n","   test_results.append(acc_test)\n","\n","optimal_min_samples_leafs = optimal_parameter(min_samples_leafs,test_results)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5f702c1f","metadata":{"id":"5f702c1f"},"outputs":[],"source":["#=================================================-\n","#### Slide 13: Plot: min samples leaf  ####\n","\n","# Plot min_sample split.\n","line1, = plt.plot(min_samples_leafs, train_results, 'b', label= \"Train accuracy\")\n","line2, = plt.plot(min_samples_leafs, test_results, 'r', label= \"Test accuracy\")\n","\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n","plt.ylabel('Accuracy')\n","plt.xlabel('min samples leafs')\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"cf170bd3","metadata":{"id":"cf170bd3"},"outputs":[],"source":["#=================================================-\n","#### Slide 15: Optimize: max features  ####\n","\n","# Max_features:\n","max_features = list(range(1,X.shape[1]))\n","train_results = []\n","test_results = []\n","\n","for max_feature in max_features:\n","   dt = DecisionTreeClassifier(max_features=max_feature)\n","   dt.fit(X_train, y_train)\n","   train_pred = dt.predict(X_train)\n","   acc_train = accuracy_score(y_train, train_pred)\n","   # Add accuracy score to previous train results\n","   train_results.append(acc_train)\n","   y_pred = dt.predict(X_test)\n","   acc_test = accuracy_score(y_test, y_pred)\n","\n","   # Add accuracy score to previous test results\n","   test_results.append(acc_test)\n","\n","optimal_max_features = optimal_parameter(max_features,test_results)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"99de4659","metadata":{"id":"99de4659"},"outputs":[],"source":["#=================================================-\n","#### Slide 16: Plot: max features  ####\n","\n","# Plot min_sample split.\n","line1, = plt.plot(max_features, train_results, 'b', label= \"Train accuracy\")\n","line2, = plt.plot(max_features, test_results, 'r', label= \"Test accuracy\")\n","\n","plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n","plt.ylabel('Accuracy')\n","plt.xlabel('max features')\n","plt.show()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f2648806","metadata":{"id":"f2648806"},"outputs":[],"source":["#=================================================-\n","#### Slide 18: Optimized model  ####\n","\n","print(\"The optimal max depth is:\", optimal_max_depth)\n","print(\"The optimal min samples split is:\", optimal_min_samples_split)\n","print(\"The optimal min samples leaf is:\", optimal_min_samples_leafs)\n","print(\"The optimal max features is:\", optimal_max_features)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"21d4de12","metadata":{"id":"21d4de12"},"outputs":[],"source":["#=================================================-\n","#### Slide 19: Build optimized model  ####\n","\n","# Set the seed.\n","np.random.seed(1)\n","\n","# Implement the Decision Tree on X_train.\n","clf_optimized = tree.DecisionTreeClassifier(max_depth = optimal_max_depth,\n","                                            min_samples_split = optimal_min_samples_split,\n","                                            min_samples_leaf = optimal_min_samples_leafs,\n","                                            max_features = optimal_max_features)\n","\n","# We can now see our optimized features where before they were just default:\n","print(clf_optimized)\n","\n","clf_optimized_fit = clf_optimized.fit(X_train, y_train)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"1ce0382d","metadata":{"id":"1ce0382d"},"outputs":[],"source":["#=================================================-\n","#### Slide 20: Predict with optimized model  ####\n","\n","# Predict on X_test.\n","y_predict_optimized = clf_optimized_fit.predict(X_test)\n","\n","# Get the accuracy score.\n","acc_score_tree_optimized = accuracy_score(y_test, y_predict_optimized)\n","\n","print(acc_score_tree_optimized)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"426385b0","metadata":{"id":"426385b0"},"outputs":[],"source":["#=================================================-\n","#### Slide 21: Train accuracy  ####\n","\n","# Compute accuracy using training data.\n","acc_train_tree_optimized = clf_optimized_fit.score(X_train,\n","                                         y_train)\n","\n","print (\"Train Accuracy:\", acc_train_tree_optimized)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"be8ef65b","metadata":{"id":"be8ef65b"},"outputs":[],"source":["#=================================================-\n","#### Slide 23: Predict and save results  ####\n","\n","# Add the optimized model to our dataframe.\n","model_final_tree = {'metrics' : \"accuracy\" ,\n","             'values' : round(acc_score_tree_optimized,4),\n","             'model':'tree_all_variables_optimized' }\n","print(model_final_tree)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"bcd31628","metadata":{"id":"bcd31628"},"outputs":[],"source":["#=================================================-\n","#### Slide 25: Exercise  ####\n","\n","\n","\n","\n","#######################################################\n","####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n","#######################################################\n"]}],"metadata":{"language":"python","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}