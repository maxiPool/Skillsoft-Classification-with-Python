{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ba0c4d",
   "metadata": {},
   "source": [
    "## 2 LOGISTICREGRESSION/LOGISTICREGRESSION/LOGISTICREGRESSION LOGISTICREGRESSION 3 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of LogisticRegression - LogisticRegression for Tasks 1-8\n",
    "#### Task 1\n",
    "##### Import the required packages. (Revisit this question as you complete the remaining exercises)\n",
    "##### Set the working directory to data directory.\n",
    "##### Print the working directory.\n",
    "##### Import the dataset `heart_failure_clinical_records_dataset.csv` as `ex_df`.\n",
    "##### Subset the DataFrame to include only numeric and categorical columns.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345db9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "# Scikit-learn package for logistic regression.\n",
    "from sklearn import linear_model\n",
    "# Model set up and tuning packages from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Scikit-learn packages for evaluating model performance.\n",
    "from sklearn import metrics\n",
    "# Scikit-learn package for data preprocessing.\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "# Set 'main_dir' to location of the project folder.\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "# Import the dataset\n",
    "ex_df = pd.read_csv(str(data_dir)+\"/\"+ 'heart_failure_clinical_records_dataset.csv')\n",
    "# Subset data\n",
    "ex_df_subset = ex_df[['age', 'serum_sodium', 'time', 'platelets', 'ejection_fraction', 'serum_creatinine', 'creatinine_phosphokinase', 'anaemia', 'high_blood_pressure', 'smoking', 'sex', 'diabetes', 'death_event', 'id']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55ef78",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### If target is not binary, convert it to binary (Hint: calculate the mean and assign the above mean to 1 and below to 0).\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target not binary\n",
    "ex_threshold = np.mean(ex_df_subset['death_event'])\n",
    "ex_df_subset['death_event'] = np.where(ex_df_subset['death_event'] > ex_threshold, 1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce3535",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Identify the two unique classes.\n",
    "##### Convert ex_target to `bool` so that it is a binary class.\n",
    "##### Check and convert NAs.\n",
    "##### Delete columns containing either 50% or more than 50% NaN Values.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the two unique classes.\n",
    "unique_values = sorted(ex_df_subset['death_event'].unique())\n",
    "ex_df_subset['death_event'] = np.where(ex_df_subset['death_event'] == unique_values[0],  False,True)\n",
    " # Check for NAs. \n",
    "print(ex_df_subset.isnull().sum())\n",
    "# Delete columns containing either 60% or more than 60% NaN Values.\n",
    "ex_perc = 60.0\n",
    "ex_min_count =  int(((100-ex_perc)/100)*ex_df_subset.shape[0] + 1)\n",
    "ex_df_subset = ex_df_subset.dropna(axis=1, \n",
    "               thresh=ex_min_count)\n",
    "print(ex_df_subset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664ce6f",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Write a function to impute NAs in both numeric and categorical columns.\n",
    "##### Split the data into X and y .\n",
    "##### Convert the categorical data to dummy variables.\n",
    "##### Set a seed and split data into train and test sets, use a 70 train - 30 test split.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute NA in both numeric and categorical columns\n",
    "def fillna(ex_df):\n",
    "    # Fill numeric columns with mean value\n",
    "    ex_df = ex_df.fillna(ex_df.mean())    \n",
    "    # Fill categorical columns with mode value\n",
    "    ex_df = ex_df.fillna(ex_df.mode().iloc[0])\n",
    "    return ex_df\n",
    "  \n",
    "ex_df_subset = fillna(ex_df_subset)\n",
    "# Split the data into X and y \n",
    "ex_columns_to_drop_from_X = ['death_event'] + ['id']\n",
    "ex_X = ex_df_subset.drop(ex_columns_to_drop_from_X, axis=1)\n",
    "ex_y = np.array(ex_df_subset['death_event'])\n",
    "# Convert categorical variables to dummy variables\n",
    "ex_X = pd.get_dummies(ex_X, columns = ['anaemia', 'high_blood_pressure', 'smoking', 'sex', 'diabetes'], dtype=float, drop_first=True)\n",
    "print(ex_X.dtypes)\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "# Split data into train and test sets, use a 70 train - 30 test split.\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = train_test_split(ex_X, \n",
    "                                                    ex_y, \n",
    "                                                    test_size = .3)\n",
    "                                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f7ae4",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Scale the features by fitting MinMaxScaler on the training data.\n",
    "##### Set up logistic regression model.\n",
    "##### Fit the model.\n",
    "##### Predict on test data.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be11e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features.\n",
    "ex_scaler = preprocessing.MinMaxScaler()\n",
    "ex_scaler.fit(ex_X_train)\n",
    "ex_X_train_scaled = ex_scaler.transform(ex_X_train)\n",
    "ex_X_test_scaled = ex_scaler.transform(ex_X_test)\n",
    "# Set up logistic regression model.\n",
    "ex_logistic_regression_model = linear_model.LogisticRegression()\n",
    "print(ex_logistic_regression_model)  \n",
    "# Fit the model.\n",
    "ex_logistic_regression_model.fit(ex_X_train_scaled, \n",
    "                              ex_y_train)\n",
    "# Predict on test data.\n",
    "ex_predicted_values = ex_logistic_regression_model.predict(ex_X_test_scaled)\n",
    "print(ex_predicted_values[:20])                              \n",
    "                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c97e26",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Take a look at test data confusion matrix.\n",
    "##### Compute test model accuracy score.\n",
    "##### Create a list of ex_target names to interpret class assignments.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at test data confusion matrix.\n",
    "ex_conf_matrix_test = metrics.confusion_matrix(ex_y_test, ex_predicted_values)\n",
    "print(ex_conf_matrix_test)\n",
    "# Compute test model accuracy score.\n",
    "ex_test_accuracy_score = metrics.accuracy_score(ex_y_test, ex_predicted_values)\n",
    "print(\"Accuracy on test data: \", ex_test_accuracy_score)\n",
    "# Create a list of ex_target names to interpret class assignments.\n",
    "ex_target_names = ex_df_subset['death_event'].unique()\n",
    "ex_target_names = ex_target_names.tolist()\n",
    "ex_target_names = [str(x) for x in ex_target_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72005",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Print an entire classification report.\n",
    "##### Get probabilities instead of predicted values.\n",
    "##### Get probabilities of test predictions only.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an entire classification report.\n",
    "ex_class_report = metrics.classification_report(ex_y_test, \n",
    "                                             ex_predicted_values, \n",
    "                                             target_names = ex_target_names)\n",
    "print(ex_class_report)\n",
    "# Get probabilities instead of predicted values.\n",
    "ex_test_probabilities = ex_logistic_regression_model.predict_proba(ex_X_test_scaled)\n",
    "print(ex_test_probabilities[0:5, :])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_test_predictions = ex_test_probabilities[:, 1]\n",
    "print(ex_test_predictions[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee46206",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Get FPR, TPR, and threshold values.\n",
    "##### Get AUC by providing the FPR and TPR.\n",
    "##### Make an ROC curve plot.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FPR, TPR, and threshold values.\n",
    "ex_fpr, ex_tpr, ex_threshold = metrics.roc_curve(ex_y_test, ex_test_predictions) \n",
    "print(\"False positive: \", ex_fpr[:5])\n",
    "print(\"True positive: \", ex_tpr[:5])\n",
    "print(\"Threshold: \", ex_threshold[:5])\n",
    "# Get AUC by providing the FPR and TPR.\n",
    "ex_auc = metrics.auc(ex_fpr, ex_tpr)\n",
    "print(\"Area under the ROC curve: \", ex_auc)\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(ex_fpr, ex_tpr, 'b', label = 'AUC = %0.2f' % ex_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d268f",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of LogisticRegression - LogisticRegression for Tasks 9-13\n",
    "#### Task 9\n",
    "##### Compute trained model accuracy score. Complete the code by filling in the None:ex_trained_accuracy_score = logistic_regression_model.score(None, None)\n",
    "##### Create regularization penalty space with l1 and l2.\n",
    "##### Create regularization constant space.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trained model accuracy score.\n",
    "ex_trained_accuracy_score = ex_logistic_regression_model.score(ex_X_train_scaled, ex_y_train)\n",
    "print(\"Accuracy on train data: \" , ex_trained_accuracy_score)\n",
    "# Create regularization penalty space.\n",
    "ex_penalty = ['l1', 'l2']\n",
    "# Create regularization constant space.\n",
    "ex_C = np.logspace(0, 10, 10)\n",
    "print(\"Regularization constant: \", ex_C)\n",
    "# Create hyperparameter options dictionary.\n",
    "ex_hyperparameters = dict(C = ex_C, penalty = ex_penalty)\n",
    "print(ex_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30181bc",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Execute a grid search 15-fold cross-validation with above parameters.\n",
    "##### Fit CV grid search and print the best_model\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search 15-fold cross-validation with above parameters.\n",
    "ex_clf = GridSearchCV(linear_model.LogisticRegression(solver='liblinear'),\n",
    "                   ex_hyperparameters,\n",
    "                   cv = 15,                         \n",
    "                   verbose = 0)                       \n",
    "# Fit CV grid search.\n",
    "ex_best_model = ex_clf.fit(ex_X_train_scaled, ex_y_train)\n",
    "print(ex_best_model)                   \n",
    "                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374750e1",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Get best penalty and constant parameters.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be462f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best penalty and constant parameters.\n",
    "ex_penalty = ex_best_model.best_estimator_.get_params()['penalty']\n",
    "ex_constant = ex_best_model.best_estimator_.get_params()['C']\n",
    "print('Best penalty: ', ex_penalty)\n",
    "print('Best C: ', ex_constant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87062df7",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Predict on test data using best model and print it.\n",
    "##### Compute best model accuracy score and print it.\n",
    "##### Compute confusion matrix for best model and print it.\n",
    "##### Create a list of target names to interpret class assignments.\n",
    "##### Compute classification report for best model.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data using best model.\n",
    "ex_best_predicted_values = ex_best_model.predict(ex_X_test_scaled)\n",
    "print(ex_best_predicted_values)\n",
    "# Compute best model accuracy score.\n",
    "ex_best_accuracy_score = metrics.accuracy_score(ex_y_test, ex_best_predicted_values)\n",
    "print(\"Accuracy on test data (best model): \", ex_best_accuracy_score)\n",
    "# Compute trained model accuracy score.\n",
    "ex_trained_accuracy_score = ex_best_model.score(ex_X_train_scaled, ex_y_train)\n",
    "print(\"Accuracy on train data: \" , ex_trained_accuracy_score)\n",
    "# Compute confusion matrix for best model.\n",
    "ex_best_confusion_matrix = metrics.confusion_matrix(ex_y_test, ex_best_predicted_values)\n",
    "print(ex_best_confusion_matrix)\n",
    "# Create a list of target names to interpret class assignments.\n",
    "ex_target_names = ['Low value', 'High value']\n",
    "# Compute classification report for best model.\n",
    "ex_best_class_report = metrics.classification_report(ex_y_test, ex_best_predicted_values,target_names = ex_target_names)\n",
    "print(ex_best_class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4f1b7",
   "metadata": {},
   "source": [
    "#### Task 13\n",
    "##### Get probabilities instead of predicted values.\n",
    "##### Get probabilities of test predictions only.\n",
    "##### Get ROC curve metrics.\n",
    "##### Make an ROC curve plot.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d88083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities instead of predicted values.\n",
    "ex_best_test_probabilities = ex_best_model.predict_proba(ex_X_test_scaled)\n",
    "print(ex_best_test_probabilities[0:5, ])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_best_test_predictions = ex_best_test_probabilities[:, 1]\n",
    "print(ex_best_test_predictions[0:5])\n",
    "# Get ROC curve metrics.\n",
    "ex_best_fpr, ex_best_tpr, ex_best_threshold = metrics.roc_curve(ex_y_test, ex_best_test_predictions)\n",
    "ex_best_auc = metrics.auc(ex_best_fpr, ex_best_tpr)\n",
    "print(ex_best_auc)\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(ex_fpr, ex_tpr, 'blue', \n",
    "         label = 'AUC = %0.2f'%ex_auc)\n",
    "plt.plot(ex_best_fpr, ex_best_tpr, 'black', \n",
    "         label = 'AUC (best) = %0.2f'%ex_best_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
